{
    "dataset_reader": {
        "type": "seq2seq",
        "source_tokenizer": {
            "type": "word",
            "word_splitter": {
                "type": "just_spaces"
            }
        },
        "target_tokenizer": {
            "type": "word",
            "word_splitter": {
                "type": "just_spaces"
            }
        },
        "source_token_indexers": {
            "tokens": {
                "namespace": "source_tokens"
            }
        },
        "target_token_indexers": {
            "tokens": {
                "namespace": "target_tokens"
            }
        }
    },
    # TODO: split data files
    "train_data_path": "data/tanaka/simplified/tanaka_featureless.txt",
    "validation_data_path": "data/tanaka/simplified/tanaka_featureless.txt",
    "model": {
        "type": "simple_seq2seq",
        "source_embedder": {
            "tokens": {
                "type": "embedding",
                "vocab_namespace": "source_tokens",
                "embedding_dim": 50,
                "trainable": true
            }
        },
        "encoder": {
            "type": "lstm",
            "input_size": 50,
            "hidden_size": 128,
            "num_layers": 2,
            "dropout": 0.1,
            "bidirectional": true
        },

        # TODO: Update - after training
        "max_decoding_steps": 20,
        "target_namespace": "target_tokens"
    },
    "iterator": {
        "type": "bucket",
        "padding_noise": 0.1,
        "batch_size" : 32,
        "sorting_keys": [["source_tokens", "num_tokens"]]
    },
    "trainer": {
        "num_epochs": 2,
        "patience": 10,
        "cuda_device": -1,
        "optimizer": {
            "type": "adam",
            "lr": 0.01
        }
    }
}
